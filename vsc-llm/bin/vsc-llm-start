#!/bin/bash

# check if a job is active
if [ ! -z "$SLURM_JOB_ID" ]; then
  echo "SLURM job active (ID: $SLURM_JOB_ID)"
  echo "run 'vsc-llm-chat' to start chat"
  exit 0
fi

# autodetec account (broken)
ACCOUNT=${1}
if [ -z "$ACCOUNT" ]; then
  echo "checking available credit accounts..."
  ACCOUNTS=$(sam-balance 2>/dev/null | grep -v "^ID" | grep -v "^===" | awk '{print $2}')
  ACCOUNT=$(echo "$ACCOUNTS" | head -1)

  if [ -z "$ACCOUNT" ]; then
      echo "no credit account found"
      echo "specify: vsc-llm-start <account_name>"
      exit 1
  fi
  echo "using account: $ACCOUNT"
fi

echo ""
echo "requesting interactive GPU session"
echo ""

srun --account=${ACCOUNT} \
   --cluster=wice \
   --partition=interactive \
   --gpus-per-node=1 \
   --cpus-per-task=4 \
   --mem=16G \
   --time=4:00:00 \
   --pty bash -c '
# auto-detect VSC paths in job
if [[ "$HOME" =~ /user/leuven/([0-9]{3})/vsc([0-9]{5}) ]]; then
  VSC_GROUP="${BASH_REMATCH[1]}"
  VSC_USER="vsc${BASH_REMATCH[2]}"
  export VSC_HOME="/user/leuven/${VSC_GROUP}/${VSC_USER}"
  export VSC_DATA="/data/leuven/${VSC_GROUP}/${VSC_USER}"
  export VSC_SCRATCH="/scratch/leuven/${VSC_GROUP}/${VSC_USER}"
fi

export VSC_LLM_ROOT=${VSC_SCRATCH}/vsc-llm-data
export APPTAINER_CACHEDIR=${VSC_LLM_ROOT}/cache
export APPTAINER_TMPDIR=${VSC_LLM_ROOT}/tmp

# Load repo path from config
if [ -f "${VSC_LLM_ROOT}/config/repo_path" ]; then
  VSC_LLM_REPO=$(cat "${VSC_LLM_ROOT}/config/repo_path")
  export PATH="${VSC_LLM_REPO}/bin:${PATH}"
fi

mkdir -p /tmp/ollama_models

echo "starting Ollama server..."

# start Ollama in background with nohup (keeps it alive until server stops)
nohup apptainer run --nv \
--bind /tmp/ollama_models:/models \
--env OLLAMA_MODELS=/models \
${VSC_LLM_ROOT}/containers/ollama.sif > /tmp/ollama.log 2>&1 &

sleep 5

# check server health
if apptainer exec --nv \
--bind /tmp/ollama_models:/models \
--env OLLAMA_MODELS=/models \
${VSC_LLM_ROOT}/containers/ollama.sif ollama list > /dev/null 2>&1; then
  echo "ollama server ready"
else
  echo "Ollama server started but may still be loading"
  echo "If chat fails to load, wait a moment and try again"
fi

clear
echo " Interactive session active "
echo "================================="
echo ""
echo "to check gpu info:"
echo "  run: nvidia-smi"
echo ""
echo "to start the chat:"
echo "  run: vsc-llm-chat"
echo ""
echo "to stop and exit:"

echo "  run: vsc-llm-stop"
echo "  run: exit"
echo ""
echo "ollama logs: /tmp/ollama.log"
echo "=============================="
echo ""

bash
'