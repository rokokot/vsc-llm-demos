#!/usr/bin/env python3
import subprocess
import sys
import os

# ANSI color codes
BLUE = '\033[94m'
GREEN = '\033[92m'
RESET = '\033[0m'
BOLD = '\033[1m'

MODEL = "llama3.2:3b"
VSC_LLM_ROOT = os.environ.get('VSC_LLM_ROOT', f"{os.environ['VSC_SCRATCH']}/vsc-llm-data")
CONTAINER = f"{VSC_LLM_ROOT}/containers/ollama.sif"

# Pull model if needed (completely silent)
print(f"{BLUE}Checking model {MODEL}...{RESET}", end='', flush=True)
subprocess.run([
  "apptainer", "exec", "--nv",
  "--bind", "/tmp/ollama_models:/models",
  "--env", "OLLAMA_MODELS=/models",
  CONTAINER, "ollama", "pull", MODEL
], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
print(f" {GREEN}âœ“{RESET}")

# Clear screen and show header
print("\033[2J\033[H")  # Clear screen
print(f"{BOLD}{'=' * 60}{RESET}")
print(f"{BOLD}  VSC-LLM Chat (Llama 3.2-3B){RESET}")
print(f"  Type your message and press Enter")
print(f"  Type {BLUE}/bye{RESET} to exit")
print(f"{BOLD}{'=' * 60}{RESET}")
print()

# Start chat - just suppress stderr (has the [GIN] logs)
subprocess.call([
  "apptainer", "exec", "--nv",
  "--bind", "/tmp/ollama_models:/models",
  "--env", "OLLAMA_MODELS=/models",
  CONTAINER, "ollama", "run", MODEL
], stderr=subprocess.DEVNULL)

print(f"\n{GREEN}Goodbye!{RESET}")
