#!/usr/bin/env python3
import subprocess
import sys

MODEL = "llama3.2:3b"
CONTAINER = f"{subprocess.check_output(['bash', '-c', 'echo $VSC_LLM_ROOT'], 
text=True).strip()}/containers/ollama.sif"

# pull model if not cached
print(f"Checking model {MODEL}...", end='', flush=True)
subprocess.run([
  "apptainer", "exec", "--nv",
  "--bind", "/tmp/ollama_models:/models",
  "--env", "OLLAMA_MODELS=/models",
  CONTAINER, "ollama", "pull", MODEL
], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
print(" âœ“")

# run chat
print(f"\n{'='*60}\n  Demo Chat UI (Llama 3.2-3B)\n  Use /bye to exit prompt\n{'='*60}\n")

proc = subprocess.Popen([
  "apptainer", "exec", "--nv",
  "--bind", "/tmp/ollama_models:/models",
  "--env", "OLLAMA_MODELS=/models",
  CONTAINER, "ollama", "run", MODEL
], stderr=subprocess.DEVNULL)

proc.wait()
