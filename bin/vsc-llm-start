#!/bin/bash

# Check if we're already in a job
if [ ! -z "$SLURM_JOB_ID" ]; then
  echo "⚠️  You're already in a SLURM job (ID: $SLURM_JOB_ID)"
  echo "Run 'vsc-llm-chat' to start chatting!"
  exit 0
fi

# Get account
ACCOUNT=${1}
if [ -z "$ACCOUNT" ]; then
  echo "Checking available credit accounts..."
  ACCOUNTS=$(sam-balance 2>/dev/null | grep -v "^ID" | grep -v "^===" | awk '{print $2}')
  ACCOUNT=$(echo "$ACCOUNTS" | head -1)

  if [ -z "$ACCOUNT" ]; then
      echo "❌ No credit account found!"
      echo "Please specify: vsc-llm-start <account_name>"
      exit 1
  fi
  echo "✓ Using account: $ACCOUNT"
fi

echo ""
echo "Requesting interactive GPU session..."
echo "(This may take a few moments)"
echo ""

srun --account=${ACCOUNT} \
   --cluster=wice \
   --partition=interactive \
   --gpus-per-node=1 \
   --cpus-per-task=4 \
   --mem=16G \
   --time=4:00:00 \
   --pty bash -c '
# Auto-detect VSC paths in job
if [[ "$HOME" =~ /user/leuven/([0-9]{3})/vsc([0-9]{5}) ]]; then
  VSC_GROUP="${BASH_REMATCH[1]}"
  VSC_USER="vsc${BASH_REMATCH[2]}"
  export VSC_HOME="/user/leuven/${VSC_GROUP}/${VSC_USER}"
  export VSC_DATA="/data/leuven/${VSC_GROUP}/${VSC_USER}"
  export VSC_SCRATCH="/scratch/leuven/${VSC_GROUP}/${VSC_USER}"
fi

export VSC_LLM_ROOT=${VSC_SCRATCH}/vsc-llm-data
export APPTAINER_CACHEDIR=${VSC_LLM_ROOT}/cache
export APPTAINER_TMPDIR=${VSC_LLM_ROOT}/tmp

# Load repo path from config
if [ -f "${VSC_LLM_ROOT}/config/repo_path" ]; then
  VSC_LLM_REPO=$(cat "${VSC_LLM_ROOT}/config/repo_path")
  export PATH="${VSC_LLM_REPO}/bin:${PATH}"
fi

mkdir -p /tmp/ollama_models

echo "Starting Ollama server..."

# Start Ollama in background with nohup (keeps it alive)
nohup apptainer run --nv \
--bind /tmp/ollama_models:/models \
--env OLLAMA_MODELS=/models \
${VSC_LLM_ROOT}/containers/ollama.sif > /tmp/ollama.log 2>&1 &

# Wait for server to be ready
sleep 5

# Check if server is responding
if apptainer exec --nv \
--bind /tmp/ollama_models:/models \
--env OLLAMA_MODELS=/models \
${VSC_LLM_ROOT}/containers/ollama.sif ollama list > /dev/null 2>&1; then
  echo "✓ Ollama server is ready!"
else
  echo "⚠️  Ollama server started but may still be initializing..."
  echo "   If chat fails, wait a moment and try again."
fi

clear
echo "============================================"
echo "  ✓ GPU Session Active"
echo "============================================"
echo ""
echo "To chat with the LLM:"
echo "  → vsc-llm-chat"
echo ""
echo "To stop and exit:"
echo "  → vsc-llm-stop"
echo "  → exit"
echo ""
echo "Ollama logs: /tmp/ollama.log"
echo "============================================"
echo ""

bash
'